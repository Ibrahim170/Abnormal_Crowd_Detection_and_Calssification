{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nnp.random.seed(333)\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-30T21:04:27.808455Z","iopub.execute_input":"2021-12-30T21:04:27.808943Z","iopub.status.idle":"2021-12-30T21:04:27.812999Z","shell.execute_reply.started":"2021-12-30T21:04:27.808911Z","shell.execute_reply":"2021-12-30T21:04:27.81232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install gdown","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:27.984167Z","iopub.execute_input":"2021-12-30T21:04:27.984414Z","iopub.status.idle":"2021-12-30T21:04:27.988381Z","shell.execute_reply.started":"2021-12-30T21:04:27.984387Z","shell.execute_reply":"2021-12-30T21:04:27.987724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gdrive_folder_link=\"https://drive.google.com/drive/folders/1RpD6itBuWYCdFeTjVilt9CKPgDvqoitq?usp=sharing\"","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:28.151386Z","iopub.execute_input":"2021-12-30T21:04:28.151652Z","iopub.status.idle":"2021-12-30T21:04:28.155069Z","shell.execute_reply.started":"2021-12-30T21:04:28.151625Z","shell.execute_reply":"2021-12-30T21:04:28.154325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import gdown\n# gdown.download_folder(gdrive_folder_link, quiet=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:28.309379Z","iopub.execute_input":"2021-12-30T21:04:28.31018Z","iopub.status.idle":"2021-12-30T21:04:28.313335Z","shell.execute_reply.started":"2021-12-30T21:04:28.310145Z","shell.execute_reply":"2021-12-30T21:04:28.312425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport time\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:28.477409Z","iopub.execute_input":"2021-12-30T21:04:28.477772Z","iopub.status.idle":"2021-12-30T21:04:28.482681Z","shell.execute_reply.started":"2021-12-30T21:04:28.477744Z","shell.execute_reply":"2021-12-30T21:04:28.481936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nprint('Found GPU at: {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:28.688441Z","iopub.execute_input":"2021-12-30T21:04:28.688708Z","iopub.status.idle":"2021-12-30T21:04:28.693898Z","shell.execute_reply.started":"2021-12-30T21:04:28.688679Z","shell.execute_reply":"2021-12-30T21:04:28.693159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# num_epochs now = 2\n\nclass ModelConfig:\n    EPOCHS = 10\n    BATCH_SIZE = 4\n    num_frames_per_second = 10\n    SEQUENCE_SIZE = 16\n    H = 256\n    W = 256\n    C = 1\n    TO_GRAY = True\n    overlapping = 0\n    rootdir = \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos\"\n    TRAIN_SAMPLE_NPZ_DIRECTORY = \"./npz_files\"\n    TEST_SAMPLE_NPZ_DIRECTORY = \"./npz_files\"\n    types = {\"Normal\":0, \"Abnormal\":1}\n    classes = {\"Explosion\":1, 'Burglary':2, 'Fighting':3, 'Assault':4, 'Arrest':5, 'Arson':6, 'Abuse':7}\n    extension = \"mp4\"\n    MODEL_WEIGHTS_DIRECTORY = \"./model_weights\"\n    COMBINE_MODEL_PATH = \"combined_model_weights.hdf5\"\n    GENERATOR_MODEL_PATH = \"generator_model_weights.hdf5\"\n    DISCRIMINATOR_MODEL_PATH = \"discriminator_model_weights.hdf5\"\n    CLASSIFIER_MODEL_PATH = \"classifier_model_weights.hdf5\"\n    AUTOENCODER_MODEL_PATH = \"autoencoder_model.hdf5\"","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:28.793242Z","iopub.execute_input":"2021-12-30T21:04:28.794165Z","iopub.status.idle":"2021-12-30T21:04:28.802025Z","shell.execute_reply.started":"2021-12-30T21:04:28.794113Z","shell.execute_reply":"2021-12-30T21:04:28.801318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir(ModelConfig.TRAIN_SAMPLE_NPZ_DIRECTORY)\nos.mkdir(ModelConfig.MODEL_WEIGHTS_DIRECTORY)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:28.986825Z","iopub.execute_input":"2021-12-30T21:04:28.987182Z","iopub.status.idle":"2021-12-30T21:04:28.991508Z","shell.execute_reply.started":"2021-12-30T21:04:28.987148Z","shell.execute_reply":"2021-12-30T21:04:28.990852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_video_times(cap):\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    duration = frame_count / fps\n\n    print('fps = ' + str(fps))\n    print('number of frames = ' + str(frame_count))\n    print('duration (S) = ' + str(duration))\n    minutes = int(duration / 60)\n    seconds = duration % 60\n    print('duration (M:S) = ' + str(minutes) + ':' + str(seconds))","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:33.052532Z","iopub.execute_input":"2021-12-30T21:04:33.05295Z","iopub.status.idle":"2021-12-30T21:04:33.058251Z","shell.execute_reply.started":"2021-12-30T21:04:33.052912Z","shell.execute_reply":"2021-12-30T21:04:33.057719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SaveVideo2Npz(file_path, npz_directory, resize=(ModelConfig.H, ModelConfig.W), \n                  num_target_frames=ModelConfig.SEQUENCE_SIZE, overlapping=0):\n    cap = cv2.VideoCapture(file_path)\n    file_name = file_path.split('/')[-1]\n    file_name = file_name.split('.')[0]\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    #get_video_times(cap)\n    len_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    segments_path_list = []\n    if fps <= ModelConfig.num_frames_per_second:\n        step = fps\n    else:\n        step = int(fps / ModelConfig.num_frames_per_second)\n    try:\n        frames = []\n        num_sampled_video = 0\n        frame_index = 0\n        for i in range(0, len_frames):\n            _, frame = cap.read()\n            if i % step == 0:\n                frame = cv2.resize(frame, resize, interpolation=cv2.INTER_AREA)\n                if ModelConfig.TO_GRAY:\n                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n                    frame = frame.reshape((ModelConfig.H, ModelConfig.W, ModelConfig.C))\n                else:\n                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n                frame = np.array(frame, dtype=np.float32)\n                frame /= 255.0\n                frames.append(frame)\n                frame_index += 1\n\n                if frame_index == num_target_frames:                    \n                    segments_path = os.path.join(npz_directory, file_name + \"_{}.npz\".format(num_sampled_video))\n                    num_sampled_video += 1\n                    np.savez(segments_path, np.array(frames))\n                    segments_path_list.append(segments_path)\n                    if overlapping == 0:\n                        frames.clear()\n                        frame_index = 0\n                    else:\n                        for _ in range(0, overlapping):\n                            frames.pop(0)\n                            frame_index -= 1\n    except Exception as e:\n        print(e)\n    finally:\n        cap.release()\n\n    return np.array(segments_path_list), fps","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:33.224095Z","iopub.execute_input":"2021-12-30T21:04:33.22442Z","iopub.status.idle":"2021-12-30T21:04:33.23817Z","shell.execute_reply.started":"2021-12-30T21:04:33.224391Z","shell.execute_reply":"2021-12-30T21:04:33.237168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# toooooooooooooooooooooooooo much memory\n\nskip_files = [\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Burglary/Burglary064_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Explosion/Explosion046_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arrest/Arrest047_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Fighting/Fighting041_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arson/Arson019_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Normal-Videos-Part-1/Normal_Videos_940_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Burglary/Burglary095_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Normal-Videos-Part-1/Normal_Videos_935_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Normal-Videos-Part-1/Normal_Videos_924_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arrest/Arrest049_x264.mp4\"\n]","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:33.393739Z","iopub.execute_input":"2021-12-30T21:04:33.393996Z","iopub.status.idle":"2021-12-30T21:04:33.39905Z","shell.execute_reply.started":"2021-12-30T21:04:33.393969Z","shell.execute_reply":"2021-12-30T21:04:33.398349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# segments, _ = SaveVideo2Npz(\"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arrest/Arrest049_x264.mp4\",\n#                             overlapping=ModelConfig.overlapping)\n\n# print(segments)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:33.558351Z","iopub.execute_input":"2021-12-30T21:04:33.559034Z","iopub.status.idle":"2021-12-30T21:04:33.562388Z","shell.execute_reply.started":"2021-12-30T21:04:33.558987Z","shell.execute_reply":"2021-12-30T21:04:33.561823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_npz_file(file_path):\n    dict_data = np.load(file_path)\n    data = dict_data['arr_0']\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:33.961826Z","iopub.execute_input":"2021-12-30T21:04:33.962398Z","iopub.status.idle":"2021-12-30T21:04:33.967098Z","shell.execute_reply.started":"2021-12-30T21:04:33.962348Z","shell.execute_reply":"2021-12-30T21:04:33.966426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# frames = read_npz_file(segments[3])\n# plt.imshow(frames[1], cmap=\"gray\")\n","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:34.139829Z","iopub.execute_input":"2021-12-30T21:04:34.140125Z","iopub.status.idle":"2021-12-30T21:04:34.143693Z","shell.execute_reply.started":"2021-12-30T21:04:34.140095Z","shell.execute_reply":"2021-12-30T21:04:34.143042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clear_npz_directory(directorty_path):\n    for f in os.listdir(directorty_path):\n        os.remove(os.path.join(directorty_path, f))\n        \nclear_npz_directory(ModelConfig.TRAIN_SAMPLE_NPZ_DIRECTORY)     ","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:34.339229Z","iopub.execute_input":"2021-12-30T21:04:34.339545Z","iopub.status.idle":"2021-12-30T21:04:34.344913Z","shell.execute_reply.started":"2021-12-30T21:04:34.339514Z","shell.execute_reply":"2021-12-30T21:04:34.343804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_name_list = []\nfile_path_list = []\nfile_type_list = []\nfile_class_list = []\nfile_npy_path_list = []","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:34.828746Z","iopub.execute_input":"2021-12-30T21:04:34.82926Z","iopub.status.idle":"2021-12-30T21:04:34.833437Z","shell.execute_reply.started":"2021-12-30T21:04:34.829214Z","shell.execute_reply":"2021-12-30T21:04:34.832697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_dataset_df():\n    global file_name_list \n    global file_path_list\n    global file_type_list \n    global file_class_list \n    global file_npy_path_list\n\n    file_name_list.clear()\n    file_path_list.clear()\n    file_type_list.clear()\n    file_class_list.clear()\n    file_npy_path_list.clear()\n\n\n    for root, subdirs, files in os.walk(ModelConfig.rootdir):\n        for filename in files:\n            if filename.split('.')[-1] != ModelConfig.extension:\n                continue\n\n            file_path = os.path.join(root, filename)\n            file_name = filename.split('.')[0]\n            \n            if file_path in skip_files:\n                continue\n            \n            file_name_list.append(file_name)\n            file_path_list.append(file_path)\n            file_class = file_path.split('/')[-2]\n            #print(file_class)\n            if file_class in ModelConfig.classes.keys():\n                file_type_list.append(ModelConfig.types[\"Abnormal\"])\n                file_class_list.append(ModelConfig.classes[file_class])\n            else:\n                file_type_list.append(ModelConfig.types[\"Normal\"])\n                file_class_list.append(ModelConfig.types[\"Normal\"])\n\n            #npy_path = Save2Npy(file_path, file_name, Config.save_npy_dir)\n            #file_npy_path_list.append(npy_path)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:34.877688Z","iopub.execute_input":"2021-12-30T21:04:34.878449Z","iopub.status.idle":"2021-12-30T21:04:34.88779Z","shell.execute_reply.started":"2021-12-30T21:04:34.878403Z","shell.execute_reply":"2021-12-30T21:04:34.887302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"build_dataset_df()","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:35.305354Z","iopub.execute_input":"2021-12-30T21:04:35.305738Z","iopub.status.idle":"2021-12-30T21:04:35.397342Z","shell.execute_reply.started":"2021-12-30T21:04:35.305711Z","shell.execute_reply":"2021-12-30T21:04:35.39652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(file_path_list), len(file_name_list), len(file_type_list), len(file_class_list)#, len(file_npy_path_list)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:35.499509Z","iopub.execute_input":"2021-12-30T21:04:35.499769Z","iopub.status.idle":"2021-12-30T21:04:35.507438Z","shell.execute_reply.started":"2021-12-30T21:04:35.499737Z","shell.execute_reply":"2021-12-30T21:04:35.506751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset_df = pd.DataFrame(list(zip(file_name_list, file_path_list, file_type_list, file_class_list, file_npy_path_list)),\n#                          columns =['file_name', 'file_path', 'file_type', 'file_class', 'npy_file_path'])\n\ndataset_df = pd.DataFrame(list(zip(file_name_list, file_path_list, file_type_list, file_class_list)),\n                          columns =['file_name', 'file_path', 'file_type', 'file_class'])","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:35.723533Z","iopub.execute_input":"2021-12-30T21:04:35.723952Z","iopub.status.idle":"2021-12-30T21:04:35.734016Z","shell.execute_reply.started":"2021-12-30T21:04:35.723916Z","shell.execute_reply":"2021-12-30T21:04:35.733407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_df","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:36.576645Z","iopub.execute_input":"2021-12-30T21:04:36.577174Z","iopub.status.idle":"2021-12-30T21:04:36.596441Z","shell.execute_reply.started":"2021-12-30T21:04:36.577136Z","shell.execute_reply":"2021-12-30T21:04:36.595852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\n\n\nclass CustomDataGen(tf.keras.utils.Sequence):\n    def __init__(self, dataset_df, X_col, y_col, directory, npz_directory, shuffle=True, data_augmentation=True):\n        self.batch_size = 1\n        self.dataset_df = dataset_df\n        self.X_col = X_col\n        self.y_col = y_col\n        self.directory = directory\n        self.npz_directory = npz_directory\n        self.shuffle = shuffle\n        self.data_aug = data_augmentation\n        self.classes = np.unique(self.dataset_df[self.y_col])\n        self.num_classes =  len(self.classes)\n        self.X_path, self.Y_dict = self.search_data() \n        self.print_stats()\n        return None\n        \n    def search_data(self):\n        X_path = []\n        Y_dict = {}\n        one_hots = to_categorical(self.dataset_df[self.y_col], self.num_classes)\n        for index in range(len(self.dataset_df)):\n            X_path.append(self.dataset_df.at[index, self.X_col])\n            Y_dict[X_path[-1]] = one_hots[index]\n        return X_path, Y_dict\n    \n    def print_stats(self):\n        self.n_files = len(self.X_path)\n        self.indexes = np.arange(len(self.X_path))\n        np.random.shuffle(self.indexes)\n        print(\"Found {} files belonging to {} classes.\".format(self.n_files,self.num_classes))\n    \n    def __len__(self):\n        steps_per_epoch = np.ceil(len(self.X_path) / float(self.batch_size))\n        return int(steps_per_epoch)\n\n    def __getitem__(self, index):\n        batch_indexs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        batch_path = [self.X_path[k] for k in batch_indexs]\n        batch_x, batch_y = self.data_generation(batch_path)               \n        return batch_x, batch_y\n    \n    def get_mini_batch(self, index):\n        return self.__getitem__(index)\n\n    def on_epoch_end(self):\n        # shuffle the data at each end of epoch\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def data_generation(self, batch_path):\n        batch_x = [self.load_data(x) for x in batch_path]\n        batch_y = [self.Y_dict[x] for x in batch_path]\n\n        batch_x = np.array(batch_x)\n        batch_y = np.array(batch_y)\n        return batch_x, batch_y\n    \n    def load_data(self, path):\n        #print(path)\n        segments, _  = SaveVideo2Npz(path, self.npz_directory, \n                                     resize=(ModelConfig.H, ModelConfig.W), \n                                     num_target_frames=ModelConfig.SEQUENCE_SIZE,\n                                     overlapping=ModelConfig.overlapping)\n        return segments","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:36.788247Z","iopub.execute_input":"2021-12-30T21:04:36.78893Z","iopub.status.idle":"2021-12-30T21:04:37.931086Z","shell.execute_reply.started":"2021-12-30T21:04:36.788891Z","shell.execute_reply":"2021-12-30T21:04:37.930321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gen = CustomDataGen(dataset_df,\n                           X_col=\"file_path\",\n                           y_col=\"file_class\",\n                           directory = ModelConfig.rootdir, \n                           npz_directory = ModelConfig.TRAIN_SAMPLE_NPZ_DIRECTORY,\n                           shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:37.93288Z","iopub.execute_input":"2021-12-30T21:04:37.933184Z","iopub.status.idle":"2021-12-30T21:04:37.948617Z","shell.execute_reply.started":"2021-12-30T21:04:37.933143Z","shell.execute_reply":"2021-12-30T21:04:37.94802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gen.get_mini_batch(0)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:37.949625Z","iopub.execute_input":"2021-12-30T21:04:37.949889Z","iopub.status.idle":"2021-12-30T21:04:39.754922Z","shell.execute_reply.started":"2021-12-30T21:04:37.949852Z","shell.execute_reply":"2021-12-30T21:04:39.75412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random \n\nn = random.randint(0, len(dataset_df))\nrandomlist = random.sample(range(0, len(dataset_df)), len(dataset_df)) \nval_size = 0.1\nval_size = int(len(dataset_df) * val_size)\nval_size = randomlist[:val_size]\nvalidation_df = dataset_df.iloc[val_size].reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:39.756516Z","iopub.execute_input":"2021-12-30T21:04:39.756802Z","iopub.status.idle":"2021-12-30T21:04:39.766326Z","shell.execute_reply.started":"2021-12-30T21:04:39.756766Z","shell.execute_reply":"2021-12-30T21:04:39.765456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#validation_df","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:39.767763Z","iopub.execute_input":"2021-12-30T21:04:39.768061Z","iopub.status.idle":"2021-12-30T21:04:39.775404Z","shell.execute_reply.started":"2021-12-30T21:04:39.768023Z","shell.execute_reply":"2021-12-30T21:04:39.774744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_gen = CustomDataGen(validation_df,\n                               X_col=\"file_path\",\n                               y_col=\"file_class\",\n                               directory = ModelConfig.rootdir,\n                               npz_directory=ModelConfig.TRAIN_SAMPLE_NPZ_DIRECTORY,\n                               shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:39.77658Z","iopub.execute_input":"2021-12-30T21:04:39.776918Z","iopub.status.idle":"2021-12-30T21:04:39.787308Z","shell.execute_reply.started":"2021-12-30T21:04:39.776797Z","shell.execute_reply":"2021-12-30T21:04:39.78645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import (Input, Conv3D, Conv3DTranspose,\n                                     ConvLSTM2D)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\n\n\ndef encoder(X_input):\n    # encoder    \n    X = Conv3D(filters=128,kernel_size=(11,11,1),strides=(4,4,1),padding='same',activation='tanh')(X_input)\n\n    X = Conv3D(filters=64,kernel_size=(5,5,1),strides=(2,2,1),padding='same',activation='tanh')(X)\n\n    X = ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,padding='same',dropout=0.4,recurrent_dropout=0.3,return_sequences=True)(X)\n\n    bottleneck = ConvLSTM2D(filters=32,kernel_size=(3,3),strides=1,padding='same',dropout=0.3,return_sequences=True)(X)\n    \n    return bottleneck\n\n    \ndef decoder(bottleneck):\n    # decoder\n    X = ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,return_sequences=True, padding='same',dropout=0.5, name=\"decoder_layer\")(bottleneck)\n\n    X = Conv3DTranspose(filters=128,kernel_size=(5,5,1),strides=(2,2,1),padding='same',activation='tanh')(X)\n\n    X = Conv3DTranspose(filters=ModelConfig.C,kernel_size=(11,11,1),strides=(4,4,1),padding='same',activation='sigmoid')(X)\n\n    return X\n\ndef AutoEncoderModel(X_input):\n    autoencoder = Model(X_input, decoder(encoder(X_input)), name='AutoEncoderModel')\n    return autoencoder\n\n\ndef custom_loss(new, original):\n    reconstruct_loss = K.mean(K.square(new-original))\n    return reconstruct_loss\n\nX_input = Input(shape=(ModelConfig.H,ModelConfig.W,ModelConfig.SEQUENCE_SIZE,ModelConfig.C))\nautoEncoderModel = AutoEncoderModel(X_input)\nopt = Adam(lr=0.001)\nautoEncoderModel.compile(loss=custom_loss, optimizer=opt, metrics=['accuracy'])\nprint(autoEncoderModel.summary())","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:39.880214Z","iopub.execute_input":"2021-12-30T21:04:39.880476Z","iopub.status.idle":"2021-12-30T21:04:40.389628Z","shell.execute_reply.started":"2021-12-30T21:04:39.880448Z","shell.execute_reply":"2021-12-30T21:04:40.388639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_result_img(autoencoder_model, batch_num, X, epoch, img_seq_num=-1 ,image_idx=-1):\n    mini_batch_size = X.shape[0]\n    valid_y = np.array([1] * mini_batch_size)\n    seq = autoencoder_model.predict(X)\n    \n    X = np.transpose(X, (0, 3, 1, 2, 4))\n    seq = np.transpose(seq, (0, 3, 1, 2, 4))\n    \n    result = []\n    result.append(X[img_seq_num][image_idx])\n    result.append(seq[img_seq_num][image_idx])\n    result = np.array(result)\n    print(\"Epoch: {}, Batch_number: {}\".format(epoch, batch_num))\n    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,20))\n    for i, ax in enumerate(axs.flatten()):\n        plt.sca(ax)\n        if ModelConfig.TO_GRAY:\n            plt.imshow(result[i].reshape(ModelConfig.H, ModelConfig.W), cmap=\"gray\")\n        else:\n            plt.imshow(result[i])\n        if i % 2 == 0:\n            plt.title('Original Image: {}'.format(i+1))\n        else:\n            plt.title('Reconstructed Image: {}'.format(i+1))\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:04:41.27844Z","iopub.execute_input":"2021-12-30T21:04:41.279162Z","iopub.status.idle":"2021-12-30T21:04:41.288073Z","shell.execute_reply.started":"2021-12-30T21:04:41.279125Z","shell.execute_reply":"2021-12-30T21:04:41.287321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nfrom tensorflow.keras.utils import Progbar\n\nclass AutoEncoder():\n    def __init__(self):        \n        self.image_shape=(ModelConfig.H, ModelConfig.W,ModelConfig.SEQUENCE_SIZE, ModelConfig.C)\n\n        learning_rate=0.0002\n        beta_1=0.5    \n\n        opt1=Adam(lr=1e-4, decay=1e-5, epsilon=1e-6)\n \n        #Build and compile the generator\n        X_input = Input(shape=(self.image_shape))\n        self.auto_encoder = AutoEncoderModel(X_input)\n        self.auto_encoder.compile(loss='mse',optimizer=opt1)                   \n\n    def train_gan(self, train_gen):\n        for epoch in range(ModelConfig.EPOCHS):           \n            reconstruct_loss_sum=0\n            no_of_minibatches=0\n            progress_bar = Progbar(target=len(train_gen))\n            print(\"Epoch : \", epoch+1)\n            for i in range(len(train_gen)):\n                sample_reconstruct_loss=0               \n                X_sample, _ = train_gen.get_mini_batch(i)\n                X_sample = X_sample[0]\n                X_sample_size = X_sample.shape[0]                \n                minibatch = None\n                j = 0\n                num_segments = 0\n                while j < X_sample_size:\n                    minibatch = []\n                    mini_batch_size = min(ModelConfig.BATCH_SIZE, X_sample_size-j)\n                    seg_indxes = list(range(j, j+mini_batch_size))\n                    minibatch = np.array([read_npz_file(X_sample[index]) for index in seg_indxes])\n                    j+=mini_batch_size\n\n                    minibatch = np.transpose(minibatch, (0, 2, 3, 1, 4))\n                                      \n                    reconstruct_loss=self.auto_encoder.train_on_batch(minibatch,minibatch)\n\n                    sample_reconstruct_loss+=reconstruct_loss      \n                    num_segments += 1\n                clear_npz_directory(ModelConfig.TRAIN_SAMPLE_NPZ_DIRECTORY)\n                \n                if i % 10 == 0:\n                    visualize_result_img(self.auto_encoder, i, minibatch, epoch+1, img_seq_num=-1 ,image_idx=-1)\n                    self.auto_encoder.save_weights(os.path.join(ModelConfig.MODEL_WEIGHTS_DIRECTORY, ModelConfig.AUTOENCODER_MODEL_PATH))                   \n                \n                reconstruct_loss_sum+=(sample_reconstruct_loss/num_segments)\n                progress_bar.update(i+1, \n                                    values=[('rec_loss', (sample_reconstruct_loss/num_segments))])\n                print()\n\n            reconstruct_loss=reconstruct_loss_sum/len(train_gen)\n\n            print(\"(^|^)  ('|')\")\n            print(\"%d %f is reconstruction loss]\" % \n                  (epoch+1,                    \n                   reconstruct_loss))\n            \n            self.auto_encoder.save_weights(os.path.join(ModelConfig.MODEL_WEIGHTS_DIRECTORY, ModelConfig.AUTOENCODER_MODEL_PATH))\n            train_gen.on_epoch_end()","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:08:38.19085Z","iopub.execute_input":"2021-12-30T21:08:38.191205Z","iopub.status.idle":"2021-12-30T21:08:38.204491Z","shell.execute_reply.started":"2021-12-30T21:08:38.191179Z","shell.execute_reply":"2021-12-30T21:08:38.204012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auto_encoder_model = AutoEncoder()","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:08:38.545521Z","iopub.execute_input":"2021-12-30T21:08:38.545881Z","iopub.status.idle":"2021-12-30T21:08:38.888528Z","shell.execute_reply.started":"2021-12-30T21:08:38.545853Z","shell.execute_reply":"2021-12-30T21:08:38.887932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(auto_encoder_model.auto_encoder.summary())","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:08:39.187373Z","iopub.execute_input":"2021-12-30T21:08:39.187595Z","iopub.status.idle":"2021-12-30T21:08:39.197623Z","shell.execute_reply.started":"2021-12-30T21:08:39.187571Z","shell.execute_reply":"2021-12-30T21:08:39.196959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#auto_encoder_model.auto_encoder.load_weights(os.path.join(ModelConfig.MODEL_WEIGHTS_DIRECTORY, ModelConfig.AUTOENCODER_MODEL_PATH))","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:08:40.024175Z","iopub.execute_input":"2021-12-30T21:08:40.024444Z","iopub.status.idle":"2021-12-30T21:08:40.029634Z","shell.execute_reply.started":"2021-12-30T21:08:40.024417Z","shell.execute_reply":"2021-12-30T21:08:40.029181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.config.optimizer.set_experimental_options({'layout_optimizer': False})\n\nwith tf.device(device_name):\n    auto_encoder_model = AutoEncoder()\n    auto_encoder_model.train_gan(train_gen)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T21:08:40.572264Z","iopub.execute_input":"2021-12-30T21:08:40.572855Z","iopub.status.idle":"2021-12-30T21:18:35.924922Z","shell.execute_reply.started":"2021-12-30T21:08:40.572809Z","shell.execute_reply":"2021-12-30T21:18:35.923864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}