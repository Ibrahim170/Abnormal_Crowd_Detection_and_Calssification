{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nnp.random.seed(333)\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-01T16:40:45.507639Z","iopub.execute_input":"2022-01-01T16:40:45.507910Z","iopub.status.idle":"2022-01-01T16:40:45.537182Z","shell.execute_reply.started":"2022-01-01T16:40:45.507829Z","shell.execute_reply":"2022-01-01T16:40:45.536565Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# !pip install gdown","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:40:45.538654Z","iopub.execute_input":"2022-01-01T16:40:45.538949Z","iopub.status.idle":"2022-01-01T16:40:45.542930Z","shell.execute_reply.started":"2022-01-01T16:40:45.538913Z","shell.execute_reply":"2022-01-01T16:40:45.542090Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# gdrive_folder_link=\"https://drive.google.com/drive/folders/1RpD6itBuWYCdFeTjVilt9CKPgDvqoitq?usp=sharing\"","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:40:45.545370Z","iopub.execute_input":"2022-01-01T16:40:45.546236Z","iopub.status.idle":"2022-01-01T16:40:45.551680Z","shell.execute_reply.started":"2022-01-01T16:40:45.546174Z","shell.execute_reply":"2022-01-01T16:40:45.550905Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# import gdown\n# gdown.download_folder(gdrive_folder_link, quiet=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:40:45.553499Z","iopub.execute_input":"2022-01-01T16:40:45.553911Z","iopub.status.idle":"2022-01-01T16:40:45.559691Z","shell.execute_reply.started":"2022-01-01T16:40:45.553877Z","shell.execute_reply":"2022-01-01T16:40:45.558974Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport time\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:40:45.561463Z","iopub.execute_input":"2022-01-01T16:40:45.561868Z","iopub.status.idle":"2022-01-01T16:40:45.773464Z","shell.execute_reply.started":"2022-01-01T16:40:45.561810Z","shell.execute_reply":"2022-01-01T16:40:45.772747Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nprint('Found GPU at: {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:40:45.775068Z","iopub.execute_input":"2022-01-01T16:40:45.775467Z","iopub.status.idle":"2022-01-01T16:40:51.989784Z","shell.execute_reply.started":"2022-01-01T16:40:45.775429Z","shell.execute_reply":"2022-01-01T16:40:51.988936Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"### num_epochs now = 2\n\nclass ModelConfig:\n    EPOCHS = 10\n    BATCH_SIZE = 4\n    num_frames_per_second = 10\n    SEQUENCE_SIZE = 16\n    H = 128\n    W = 128\n    C = 3\n    TO_GRAY = False\n    overlapping = 0\n    rootdir = \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos\"\n    TRAIN_SAMPLE_NPZ_DIRECTORY = \"./npz_files\"\n    TEST_SAMPLE_NPZ_DIRECTORY = \"./npz_files\"\n    types = {\"Normal\":0, \"Abnormal\":1}\n    classes = {\"Explosion\":1, 'Burglary':2, 'Fighting':3, 'Assault':4, 'Arrest':5, 'Arson':6, 'Abuse':7}\n    extension = \"mp4\"\n    MODEL_WEIGHTS_DIRECTORY = \"./model_weights\"\n    COMBINE_MODEL_PATH = \"combined_model_weights.hdf5\"\n    GENERATOR_MODEL_PATH = \"generator_model_weights.hdf5\"\n    DISCRIMINATOR_MODEL_PATH = \"discriminator_model_weights.hdf5\"\n    CLASSIFIER_MODEL_PATH = \"classifier_model_weights.hdf5\"\n    AUTOENCODER_MODEL_PATH = \"autoencoder_model.hdf5\"","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:03.791565Z","iopub.execute_input":"2022-01-01T16:41:03.792015Z","iopub.status.idle":"2022-01-01T16:41:03.797883Z","shell.execute_reply.started":"2022-01-01T16:41:03.791978Z","shell.execute_reply":"2022-01-01T16:41:03.797245Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"os.mkdir(ModelConfig.TRAIN_SAMPLE_NPZ_DIRECTORY)\nos.mkdir(ModelConfig.MODEL_WEIGHTS_DIRECTORY)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:04.179290Z","iopub.execute_input":"2022-01-01T16:41:04.179751Z","iopub.status.idle":"2022-01-01T16:41:04.183381Z","shell.execute_reply.started":"2022-01-01T16:41:04.179714Z","shell.execute_reply":"2022-01-01T16:41:04.182723Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_video_times(cap):\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    duration = frame_count / fps\n\n    print('fps = ' + str(fps))\n    print('number of frames = ' + str(frame_count))\n    print('duration (S) = ' + str(duration))\n    minutes = int(duration / 60)\n    seconds = duration % 60\n    print('duration (M:S) = ' + str(minutes) + ':' + str(seconds))","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:04.686888Z","iopub.execute_input":"2022-01-01T16:41:04.687481Z","iopub.status.idle":"2022-01-01T16:41:04.693575Z","shell.execute_reply.started":"2022-01-01T16:41:04.687442Z","shell.execute_reply":"2022-01-01T16:41:04.692630Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def crop_image_black_background(img):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n    hh, ww = thresh.shape\n    thresh[hh:hh, 0:ww] = 0\n    white = np.where(thresh == 255)\n    \n    if len(white[0]) <= 1 or len(white[1]) <= 1:\n        return None\n    \n    xmin, ymin, xmax, ymax = np.min(white[1]), np.min(white[0]), np.max(white[1]), np.max(white[0])\n    crop = img[ymin:ymax, xmin:xmax]\n    \n    if (crop.shape[0] < img.shape[0]/2) and (crop.shape[1] < img.shape[1]/2):\n        return None\n    return crop","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:07.265061Z","iopub.execute_input":"2022-01-01T16:41:07.265775Z","iopub.status.idle":"2022-01-01T16:41:07.272996Z","shell.execute_reply.started":"2022-01-01T16:41:07.265731Z","shell.execute_reply":"2022-01-01T16:41:07.272218Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SaveVideo2Npz(file_path, npz_directory, resize=(ModelConfig.H, ModelConfig.W), \n                  num_target_frames=ModelConfig.SEQUENCE_SIZE, overlapping=0):\n    global error_frame\n    cap = cv2.VideoCapture(file_path)\n    file_name = file_path.split('/')[-1]\n    file_name = file_name.split('.')[0]\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    #get_video_times(cap)\n    len_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    segments_path_list = []\n    if fps <= ModelConfig.num_frames_per_second:\n        step = fps\n    else:\n        step = int(fps / ModelConfig.num_frames_per_second)\n    try:\n        frames = []\n        num_sampled_video = 0\n        frame_index = 0\n        for i in range(0, len_frames):\n            _, frame = cap.read()\n            frame = crop_image_black_background(frame)\n            if frame is None or (frame.shape[0] <= 0 or frame.shape[1] <= 0):\n                continue\n                \n            if i % step == 0:\n                frame = cv2.resize(frame, resize, interpolation=cv2.INTER_AREA)\n                if ModelConfig.TO_GRAY:\n                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n                    frame = frame.reshape((ModelConfig.H, ModelConfig.W, ModelConfig.C))\n                else:\n                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n                frame = np.array(frame, dtype=np.float32)\n                frame /= 255.0\n                frames.append(frame)\n                frame_index += 1\n\n                if frame_index == num_target_frames:                    \n                    segments_path = os.path.join(npz_directory, file_name + \"_{}.npz\".format(num_sampled_video))\n                    num_sampled_video += 1\n                    np.savez(segments_path, np.array(frames))\n                    segments_path_list.append(segments_path)\n                    if overlapping == 0:\n                        frames.clear()\n                        frame_index = 0\n                    else:\n                        for _ in range(0, overlapping):\n                            frames.pop(0)\n                            frame_index -= 1\n    except Exception as e:\n        print(e)\n    finally:\n        cap.release()\n\n    return np.array(segments_path_list), fps","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:09.264117Z","iopub.execute_input":"2022-01-01T16:41:09.264652Z","iopub.status.idle":"2022-01-01T16:41:09.279292Z","shell.execute_reply.started":"2022-01-01T16:41:09.264617Z","shell.execute_reply":"2022-01-01T16:41:09.278343Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# toooooooooooooooooooooooooo much memory\n\nskip_files = [\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Burglary/Burglary064_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Explosion/Explosion046_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arrest/Arrest047_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Fighting/Fighting041_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arson/Arson019_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Normal-Videos-Part-1/Normal_Videos_940_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Burglary/Burglary095_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Normal-Videos-Part-1/Normal_Videos_935_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Normal-Videos-Part-1/Normal_Videos_924_x264.mp4\",\n    \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arrest/Arrest049_x264.mp4\"\n]","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:10.849959Z","iopub.execute_input":"2022-01-01T16:41:10.850667Z","iopub.status.idle":"2022-01-01T16:41:10.855456Z","shell.execute_reply.started":"2022-01-01T16:41:10.850630Z","shell.execute_reply":"2022-01-01T16:41:10.854444Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# segments, _ = SaveVideo2Npz(\"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arrest/Arrest049_x264.mp4\",\n#                             overlapping=ModelConfig.overlapping)\n\n# print(segments)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:11.678758Z","iopub.execute_input":"2022-01-01T16:41:11.679471Z","iopub.status.idle":"2022-01-01T16:41:11.685149Z","shell.execute_reply.started":"2022-01-01T16:41:11.679432Z","shell.execute_reply":"2022-01-01T16:41:11.684384Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def read_npz_file(file_path):\n    dict_data = np.load(file_path)\n    data = dict_data['arr_0']\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:12.553851Z","iopub.execute_input":"2022-01-01T16:41:12.554095Z","iopub.status.idle":"2022-01-01T16:41:12.557826Z","shell.execute_reply.started":"2022-01-01T16:41:12.554067Z","shell.execute_reply":"2022-01-01T16:41:12.557192Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# frames = read_npz_file(segments[3])\n# plt.imshow(frames[1], cmap=\"gray\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:13.347039Z","iopub.execute_input":"2022-01-01T16:41:13.347380Z","iopub.status.idle":"2022-01-01T16:41:13.351051Z","shell.execute_reply.started":"2022-01-01T16:41:13.347344Z","shell.execute_reply":"2022-01-01T16:41:13.350360Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def clear_npz_directory(directorty_path):\n    for f in os.listdir(directorty_path):\n        os.remove(os.path.join(directorty_path, f))\n        \nclear_npz_directory(ModelConfig.TRAIN_SAMPLE_NPZ_DIRECTORY)     ","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:14.139568Z","iopub.execute_input":"2022-01-01T16:41:14.139821Z","iopub.status.idle":"2022-01-01T16:41:14.144952Z","shell.execute_reply.started":"2022-01-01T16:41:14.139792Z","shell.execute_reply":"2022-01-01T16:41:14.143914Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"file_name_list = []\nfile_path_list = []\nfile_type_list = []\nfile_class_list = []\nfile_npy_path_list = []","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:14.365235Z","iopub.execute_input":"2022-01-01T16:41:14.365612Z","iopub.status.idle":"2022-01-01T16:41:14.369557Z","shell.execute_reply.started":"2022-01-01T16:41:14.365580Z","shell.execute_reply":"2022-01-01T16:41:14.368867Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def build_dataset_df():\n    global file_name_list \n    global file_path_list\n    global file_type_list \n    global file_class_list \n    global file_npy_path_list\n\n    file_name_list.clear()\n    file_path_list.clear()\n    file_type_list.clear()\n    file_class_list.clear()\n    file_npy_path_list.clear()\n\n\n    for root, subdirs, files in os.walk(ModelConfig.rootdir):\n        for filename in files:\n            if filename.split('.')[-1] != ModelConfig.extension:\n                continue\n\n            file_path = os.path.join(root, filename)\n            file_name = filename.split('.')[0]\n            \n            if file_path in skip_files:\n                continue\n            \n            file_name_list.append(file_name)\n            file_path_list.append(file_path)\n            file_class = file_path.split('/')[-2]\n            #print(file_class)\n            if file_class in ModelConfig.classes.keys():\n                file_type_list.append(ModelConfig.types[\"Abnormal\"])\n                file_class_list.append(ModelConfig.classes[file_class])\n            else:\n                file_type_list.append(ModelConfig.types[\"Normal\"])\n                file_class_list.append(ModelConfig.types[\"Normal\"])\n\n            #npy_path = Save2Npy(file_path, file_name, Config.save_npy_dir)\n            #file_npy_path_list.append(npy_path)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:14.800668Z","iopub.execute_input":"2022-01-01T16:41:14.801169Z","iopub.status.idle":"2022-01-01T16:41:14.809325Z","shell.execute_reply.started":"2022-01-01T16:41:14.801130Z","shell.execute_reply":"2022-01-01T16:41:14.808572Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"build_dataset_df()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:15.298628Z","iopub.execute_input":"2022-01-01T16:41:15.299248Z","iopub.status.idle":"2022-01-01T16:41:15.347530Z","shell.execute_reply.started":"2022-01-01T16:41:15.299188Z","shell.execute_reply":"2022-01-01T16:41:15.346691Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"len(file_path_list), len(file_name_list), len(file_type_list), len(file_class_list)#, len(file_npy_path_list)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:15.630643Z","iopub.execute_input":"2022-01-01T16:41:15.631310Z","iopub.status.idle":"2022-01-01T16:41:15.638637Z","shell.execute_reply.started":"2022-01-01T16:41:15.631263Z","shell.execute_reply":"2022-01-01T16:41:15.637803Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#dataset_df = pd.DataFrame(list(zip(file_name_list, file_path_list, file_type_list, file_class_list, file_npy_path_list)),\n#                          columns =['file_name', 'file_path', 'file_type', 'file_class', 'npy_file_path'])\n\ndataset_df = pd.DataFrame(list(zip(file_name_list, file_path_list, file_type_list, file_class_list)),\n                          columns =['file_name', 'file_path', 'file_type', 'file_class'])","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:16.220537Z","iopub.execute_input":"2022-01-01T16:41:16.220787Z","iopub.status.idle":"2022-01-01T16:41:16.230074Z","shell.execute_reply.started":"2022-01-01T16:41:16.220753Z","shell.execute_reply":"2022-01-01T16:41:16.229262Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"dataset_df","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:16.951486Z","iopub.execute_input":"2022-01-01T16:41:16.951734Z","iopub.status.idle":"2022-01-01T16:41:16.973010Z","shell.execute_reply.started":"2022-01-01T16:41:16.951707Z","shell.execute_reply":"2022-01-01T16:41:16.972384Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\n\n\nclass CustomDataGen(tf.keras.utils.Sequence):\n    def __init__(self, dataset_df, X_col, y_col, directory, npz_directory, shuffle=True, data_augmentation=True):\n        self.batch_size = 1\n        self.dataset_df = dataset_df\n        self.X_col = X_col\n        self.y_col = y_col\n        self.directory = directory\n        self.npz_directory = npz_directory\n        self.shuffle = shuffle\n        self.data_aug = data_augmentation\n        self.classes = np.unique(self.dataset_df[self.y_col])\n        self.num_classes =  len(self.classes)\n        self.X_path, self.Y_dict = self.search_data() \n        self.print_stats()\n        return None\n        \n    def search_data(self):\n        X_path = []\n        Y_dict = {}\n        one_hots = to_categorical(self.dataset_df[self.y_col], self.num_classes)\n        for index in range(len(self.dataset_df)):\n            X_path.append(self.dataset_df.at[index, self.X_col])\n            Y_dict[X_path[-1]] = one_hots[index]\n        return X_path, Y_dict\n    \n    def print_stats(self):\n        self.n_files = len(self.X_path)\n        self.indexes = np.arange(len(self.X_path))\n        np.random.shuffle(self.indexes)\n        print(\"Found {} files belonging to {} classes.\".format(self.n_files,self.num_classes))\n    \n    def __len__(self):\n        steps_per_epoch = np.ceil(len(self.X_path) / float(self.batch_size))\n        return int(steps_per_epoch)\n\n    def __getitem__(self, index):\n        batch_indexs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        batch_path = [self.X_path[k] for k in batch_indexs]\n        batch_x, batch_y = self.data_generation(batch_path)               \n        return batch_x, batch_y\n    \n    def get_mini_batch(self, index):\n        return self.__getitem__(index)\n\n    def on_epoch_end(self):\n        # shuffle the data at each end of epoch\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def data_generation(self, batch_path):\n        batch_x = [self.load_data(x) for x in batch_path]\n        batch_y = [self.Y_dict[x] for x in batch_path]\n\n        batch_x = np.array(batch_x)\n        batch_y = np.array(batch_y)\n        return batch_x, batch_y\n    \n    def load_data(self, path):\n        #print(path)\n        segments, _  = SaveVideo2Npz(path, self.npz_directory, \n                                     resize=(ModelConfig.H, ModelConfig.W), \n                                     num_target_frames=ModelConfig.SEQUENCE_SIZE,\n                                     overlapping=ModelConfig.overlapping)\n        return segments","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:17.570433Z","iopub.execute_input":"2022-01-01T16:41:17.570743Z","iopub.status.idle":"2022-01-01T16:41:18.616864Z","shell.execute_reply.started":"2022-01-01T16:41:17.570707Z","shell.execute_reply":"2022-01-01T16:41:18.616124Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_gen = CustomDataGen(dataset_df,\n                           X_col=\"file_path\",\n                           y_col=\"file_class\",\n                           directory = ModelConfig.rootdir, \n                           npz_directory = ModelConfig.TRAIN_SAMPLE_NPZ_DIRECTORY,\n                           shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:18.618191Z","iopub.execute_input":"2022-01-01T16:41:18.618459Z","iopub.status.idle":"2022-01-01T16:41:18.637076Z","shell.execute_reply.started":"2022-01-01T16:41:18.618426Z","shell.execute_reply":"2022-01-01T16:41:18.636255Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_gen.get_mini_batch(0)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:18.780670Z","iopub.execute_input":"2022-01-01T16:41:18.781015Z","iopub.status.idle":"2022-01-01T16:41:22.027710Z","shell.execute_reply.started":"2022-01-01T16:41:18.780984Z","shell.execute_reply":"2022-01-01T16:41:22.026987Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import random \n\nn = random.randint(0, len(dataset_df))\nrandomlist = random.sample(range(0, len(dataset_df)), len(dataset_df)) \nval_size = 0.1\nval_size = int(len(dataset_df) * val_size)\nval_size = randomlist[:val_size]\nvalidation_df = dataset_df.iloc[val_size].reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:22.029167Z","iopub.execute_input":"2022-01-01T16:41:22.029879Z","iopub.status.idle":"2022-01-01T16:41:22.039183Z","shell.execute_reply.started":"2022-01-01T16:41:22.029838Z","shell.execute_reply":"2022-01-01T16:41:22.038438Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#validation_df","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:22.041563Z","iopub.execute_input":"2022-01-01T16:41:22.041765Z","iopub.status.idle":"2022-01-01T16:41:22.048656Z","shell.execute_reply.started":"2022-01-01T16:41:22.041741Z","shell.execute_reply":"2022-01-01T16:41:22.048033Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"validation_gen = CustomDataGen(validation_df,\n                               X_col=\"file_path\",\n                               y_col=\"file_class\",\n                               directory = ModelConfig.rootdir,\n                               npz_directory=ModelConfig.TRAIN_SAMPLE_NPZ_DIRECTORY,\n                               shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:22.052069Z","iopub.execute_input":"2022-01-01T16:41:22.052354Z","iopub.status.idle":"2022-01-01T16:41:22.060611Z","shell.execute_reply.started":"2022-01-01T16:41:22.052326Z","shell.execute_reply":"2022-01-01T16:41:22.059820Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import (Input, Activation, TimeDistributed,\n                                     BatchNormalization, Conv3D, Conv2D, \n                                     LeakyReLU, Conv3DTranspose, Conv2DTranspose,\n                                     ConvLSTM2D, LayerNormalization)\nfrom tensorflow.keras.layers import MaxPool3D\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.backend as K\n\n\ndef encoder(X_input):\n    # encoder    \n    X = Conv3D(32, 3, padding='same')(X_input)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    X = MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid')(X)\n\n    X = Conv3D(48, 3, padding='same')(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    X = MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid')(X)\n\n    X = Conv3D(64, 3, padding='same')(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    X = MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid')(X)\n    # current shape is 2x16x16x64\n    X = Conv3D(64, 3, padding='same')(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    bottleneck = MaxPool3D(pool_size=(2, 2, 2), strides=(1, 1, 1), padding='same')(X)\n    \n    return bottleneck\n\n    \ndef decoder(bottleneck):    \n    X = Conv3DTranspose(48, 2, strides=(2, 2, 2), padding='valid', name=\"decoder_layer\")(bottleneck)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    # current shape is 4x32x32x48\n    X = Conv3DTranspose(32, 2, strides=(2, 2, 2), padding='valid')(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    # current shape is 8x64x64x32\n    X = Conv3DTranspose(32, 2, strides=(2, 2, 2), padding='valid')(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    # current shape is 16x128x128x32\n    X = Conv3D(ModelConfig.C, 3, strides=(1, 1, 1), padding='same')(X)\n    X = Activation('sigmoid')(X)\n\n    return X\n\ndef AutoEncoderModel(X_input):\n    autoencoder = Model(X_input, decoder(encoder(X_input)), name='AutoEncoderModel')\n    return autoencoder\n\n\ndef custom_loss(new, original):\n    reconstruct_loss = K.mean(K.square(new-original))\n    return reconstruct_loss\n\nX_input = Input(shape=(ModelConfig.SEQUENCE_SIZE, ModelConfig.H,ModelConfig.W,ModelConfig.C))\nautoEncoderModel = AutoEncoderModel(X_input)\nopt = keras.optimizers.Adam(learning_rate=0.001)\nautoEncoderModel.compile(\n    loss=custom_loss, optimizer=opt, metrics=['accuracy'])\nprint(autoEncoderModel.summary())","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:22.191178Z","iopub.execute_input":"2022-01-01T16:41:22.191401Z","iopub.status.idle":"2022-01-01T16:41:22.741586Z","shell.execute_reply.started":"2022-01-01T16:41:22.191378Z","shell.execute_reply":"2022-01-01T16:41:22.740857Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.layers import Conv3D,ConvLSTM2D, Flatten, Dense, Input,BatchNormalization,Dropout\nfrom tensorflow.keras.optimizers import Adam\n\ndef create_discriminator_model():    \n    \n    model = Sequential()\n    \n    model.add(Input((ModelConfig.SEQUENCE_SIZE,ModelConfig.H,ModelConfig.W,ModelConfig.C)))\n    model.add(Conv3D(32, 3, padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU())\n    model.add(MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid'))\n    \n    model.add(Conv3D(48, 3, padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU())\n    model.add(MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid'))\n\n    model.add(Conv3D(64, 3, padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU())\n    model.add(MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid'))\n    \n    model.add(Conv3D(64, 3, padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU())    \n    model.add(MaxPool3D(pool_size=(2, 2, 2), strides=(1, 1, 1), padding='same'))\n    \n    model.add(Conv3D(32, 3, padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU())    \n    model.add(MaxPool3D(pool_size=(2, 2, 2), strides=(1, 1, 1), padding='same'))\n    \n    model.add(Conv3D(16, 3, padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU())    \n    model.add(MaxPool3D(pool_size=(2, 2, 2), strides=(1, 1, 1), padding='same'))\n    \n    model.add(Flatten())\n    model.add(Dense(128,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    return model\n\n# def DiscriminatorModel(X_input):\n#     discriminator = Model(X_input, create_discriminator_model(X_input), name='Discriminator')\n#     return discriminator\n\n#X_input = Input(shape=(ModelConfig.H,ModelConfig.W,ModelConfig.SEQUENCE_SIZE,ModelConfig.C))\ndiscriminator = create_discriminator_model()\nopt = Adam(lr=0.001)\nloss = BinaryCrossentropy()\ndiscriminator.compile(loss=loss,\n                      optimizer=opt,\n                      metrics=['accuracy'])\nprint(discriminator.summary())","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:22.743129Z","iopub.execute_input":"2022-01-01T16:41:22.743402Z","iopub.status.idle":"2022-01-01T16:41:22.930886Z","shell.execute_reply.started":"2022-01-01T16:41:22.743366Z","shell.execute_reply":"2022-01-01T16:41:22.930248Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def visualize_result(autoencoder_model, gan_model, batch_num, X, epoch, img_seq_num=-1 ,image_idx=-1):\n    mini_batch_size = X.shape[0]\n    valid_y = np.array([1] * mini_batch_size)\n    seq = autoencoder_model.predict(X)\n    y_hat = gan_model.predict(X)\n    \n    result = []\n    result.append(X[img_seq_num][image_idx])\n    result.append(seq[img_seq_num][image_idx])\n    result = np.array(result)\n    print(\"Epoch: {}, Batch_number: {}\".format(epoch, batch_num))\n    print(\"y_hat:{}\".format(round(y_hat[img_seq_num][0])))\n    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,20))\n    for i, ax in enumerate(axs.flatten()):\n        plt.sca(ax)\n        if ModelConfig.TO_GRAY:\n            plt.imshow(result[i].reshape(ModelConfig.H, ModelConfig.W), cmap=\"gray\")\n        else:\n            plt.imshow(result[i])\n        if i % 2 == 0:\n            plt.title('Original Image: {}'.format(i+1))\n        else:\n            plt.title('Reconstructed Image: {}'.format(i+1))\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:37.223342Z","iopub.execute_input":"2022-01-01T16:41:37.223595Z","iopub.status.idle":"2022-01-01T16:41:37.235412Z","shell.execute_reply.started":"2022-01-01T16:41:37.223569Z","shell.execute_reply":"2022-01-01T16:41:37.234610Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\nimport os\nfrom tensorflow.keras.utils import Progbar\n\nclass GAN():\n    def __init__(self):        \n        self.image_shape=(ModelConfig.SEQUENCE_SIZE,ModelConfig.H, ModelConfig.W, ModelConfig.C)\n\n        #learning_rate=0.0002\n        #beta_1=0.5    \n\n        opt1=Adam(learning_rate=1e-4, decay=1e-5, epsilon=1e-6)\n        opt2=Adam(learning_rate=1e-4, decay=1e-5, epsilon=1e-6)\n        opt3=Adam(learning_rate=1e-4, decay=1e-5, epsilon=1e-6)\n        \n        #Build and compile the discriminator\n        self.discriminator=create_discriminator_model()\n        self.discriminator.compile(loss=BinaryCrossentropy(),optimizer=opt1,metrics=['accuracy'])\n        \n        #Build and compile the generator\n        X_input = Input(shape=(self.image_shape))\n        self.generator = AutoEncoderModel(X_input)\n        self.generator.compile(loss='mse',optimizer=opt2)\n        \n        #the generator takes a video as input and generates a modified video\n        z = Input(shape=(self.image_shape))\n        img = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(img)\n        self.combined = Model(z, validity)\n        self.combined.compile(loss=BinaryCrossentropy(), optimizer=opt3, metrics=['accuracy'])\n\n    def re_init_combine(self, generator, discriminator):\n        self.generator = generator\n        self.discriminator = discriminator\n        z = Input(shape=(self.image_shape))\n        img = self.generator(z)\n        self.discriminator.trainable = False\n        validity = self.discriminator(img)\n        self.combined = Model(z, validity)\n        opt3=Adam(lr=1e-4, decay=1e-5, epsilon=1e-6)\n        self.combined.compile(loss=BinaryCrossentropy(), optimizer=opt3, metrics=['accuracy'])\n        \n    def load_weights(self):\n        self.generator.load_weights(os.path.join(ModelConfig.MODEL_WEIGHTS_DIRECTORY, ModelConfig.GENERATOR_MODEL_PATH))\n        self.discriminator.load_weights(os.path.join(ModelConfig.MODEL_WEIGHTS_DIRECTORY, ModelConfig.DISCRIMINATOR_MODEL_PATH))\n        self.combined.load_weights(os.path.join(ModelConfig.MODEL_WEIGHTS_DIRECTORY, ModelConfig.COMBINE_MODEL_PATH))        \n\n    def train_gan(self, train_gen):\n        for epoch in range(ModelConfig.EPOCHS):\n            d_loss_sum=np.zeros(2)\n            g_loss_sum=np.zeros(2)\n            reconstruct_loss_sum=0\n            no_of_minibatches=0\n            progress_bar = Progbar(target=len(train_gen))\n            print(\"Epoch : \", epoch+1)\n            for i in range(len(train_gen)):\n                sample_reconstruct_loss=0\n                sample_d_loss=np.zeros(2)\n                sample_g_loss=np.zeros(2)\n                X_sample, _ = train_gen.get_mini_batch(i)\n                X_sample = X_sample[0]\n                X_sample_size = X_sample.shape[0]                \n                minibatch = None\n                j = 0\n                num_segments = 0\n                while j < X_sample_size:\n                    minibatch = []\n                    mini_batch_size = min(ModelConfig.BATCH_SIZE, X_sample_size-j)\n                    seg_indxes = list(range(j, j+mini_batch_size))\n                    minibatch = np.array([read_npz_file(X_sample[index]) for index in seg_indxes])\n                    j+=mini_batch_size                                         \n                    gen_vids=self.generator.predict(minibatch)\n\n                    #might have to combine these to improve batch norm\n                    d_loss_real = self.discriminator.train_on_batch(minibatch, tf.ones((mini_batch_size,1)))\n                    d_loss_fake = self.discriminator.train_on_batch(gen_vids, tf.zeros((mini_batch_size,1)))\n                    d_loss=0.5*tf.math.add(d_loss_real,d_loss_fake)\n                    # ---------------------\n                    #  Train Generator\n                    # ---------------------\n                    # The generator wants the discriminator to label the generated samples as valid (ones)\n                    valid_y = tf.ones((mini_batch_size,1))\n                    # Train the generator\n                    g_loss, _ = self.combined.train_on_batch(minibatch,valid_y)\n\n                    reconstruct_loss=self.generator.train_on_batch(minibatch,minibatch)\n\n                    sample_d_loss += d_loss\n                    sample_g_loss += g_loss\n                    sample_reconstruct_loss += reconstruct_loss\n                    num_segments += 1\n                clear_npz_directory(ModelConfig.TRAIN_SAMPLE_NPZ_DIRECTORY)\n                \n                if i % 10 == 0:\n                    visualize_result(self.generator, self.combined, i, minibatch, epoch+1, img_seq_num=-1 ,image_idx=-1)\n                    self.generator.save_weights(os.path.join(ModelConfig.MODEL_WEIGHTS_DIRECTORY, ModelConfig.GENERATOR_MODEL_PATH))\n                    self.discriminator.save_weights(os.path.join(ModelConfig.MODEL_WEIGHTS_DIRECTORY, ModelConfig.DISCRIMINATOR_MODEL_PATH))\n                    self.combined.save_weights(os.path.join(ModelConfig.MODEL_WEIGHTS_DIRECTORY, ModelConfig.COMBINE_MODEL_PATH))\n                    \n                g_loss_sum += (sample_g_loss/num_segments)\n                d_loss_sum += (sample_d_loss/num_segments)\n                reconstruct_loss_sum += (sample_reconstruct_loss/num_segments)\n                progress_bar.update(i+1, \n                                    values=[('d_loss', (sample_d_loss[0]/num_segments)),\n                                            ('g_loss', (sample_g_loss[0]/num_segments)),\n                                            ('rec_loss', (sample_reconstruct_loss/num_segments))])\n                print()\n\n            g_loss=g_loss_sum/len(train_gen)\n            d_loss=d_loss_sum/len(train_gen)\n            reconstruct_loss=reconstruct_loss_sum/len(train_gen)\n\n            print(\"(^|^)  ('|')\")\n            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f, accuracy %.2f%% from which %f is combined loss and %f is reconstruction loss]\" % \n                  (epoch+1, \n                   d_loss[0], \n                   100*d_loss[1], \n                   g_loss[0]+reconstruct_loss,\n                   g_loss[1]*100,\n                   g_loss[0],\n                   reconstruct_loss))\n        \n            self.generator.save_weights(os.path.join(ModelConfig.MODEL_WEIGHTS_DIRECTORY, ModelConfig.GENERATOR_MODEL_PATH))\n            self.discriminator.save_weights(os.path.join(ModelConfig.MODEL_WEIGHTS_DIRECTORY, ModelConfig.DISCRIMINATOR_MODEL_PATH))\n            self.combined.save_weights(os.path.join(ModelConfig.MODEL_WEIGHTS_DIRECTORY, ModelConfig.COMBINE_MODEL_PATH))\n            train_gen.on_epoch_end()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:38.243671Z","iopub.execute_input":"2022-01-01T16:41:38.243925Z","iopub.status.idle":"2022-01-01T16:41:38.272517Z","shell.execute_reply.started":"2022-01-01T16:41:38.243894Z","shell.execute_reply":"2022-01-01T16:41:38.271835Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"gan = GAN()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:40.958139Z","iopub.execute_input":"2022-01-01T16:41:40.958795Z","iopub.status.idle":"2022-01-01T16:41:41.424732Z","shell.execute_reply.started":"2022-01-01T16:41:40.958759Z","shell.execute_reply":"2022-01-01T16:41:41.424031Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print(gan.generator.summary())\nprint(gan.discriminator.summary())\nprint(gan.combined.summary())","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:41.430764Z","iopub.execute_input":"2022-01-01T16:41:41.430962Z","iopub.status.idle":"2022-01-01T16:41:41.467191Z","shell.execute_reply.started":"2022-01-01T16:41:41.430938Z","shell.execute_reply":"2022-01-01T16:41:41.466568Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# gan.load_weights()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:42.591120Z","iopub.execute_input":"2022-01-01T16:41:42.591756Z","iopub.status.idle":"2022-01-01T16:41:42.596283Z","shell.execute_reply.started":"2022-01-01T16:41:42.591720Z","shell.execute_reply":"2022-01-01T16:41:42.595524Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"with tf.device(device_name):\n    gan = GAN()\n#     gan.load_weights()\n    gan.train_gan(train_gen)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:41:43.738102Z","iopub.execute_input":"2022-01-01T16:41:43.738488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-01-01T01:17:41.448135Z","iopub.execute_input":"2022-01-01T01:17:41.448499Z","iopub.status.idle":"2022-01-01T01:17:41.474878Z","shell.execute_reply.started":"2022-01-01T01:17:41.448418Z","shell.execute_reply":"2022-01-01T01:17:41.474228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}