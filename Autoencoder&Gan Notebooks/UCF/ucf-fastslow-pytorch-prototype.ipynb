{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-02T08:01:15.430782Z","iopub.execute_input":"2022-01-02T08:01:15.431448Z","iopub.status.idle":"2022-01-02T08:01:15.457014Z","shell.execute_reply.started":"2022-01-02T08:01:15.431351Z","shell.execute_reply":"2022-01-02T08:01:15.456318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install av","metadata":{"execution":{"iopub.status.busy":"2022-01-02T08:01:32.471567Z","iopub.execute_input":"2022-01-02T08:01:32.471839Z","iopub.status.idle":"2022-01-02T08:01:43.037701Z","shell.execute_reply.started":"2022-01-02T08:01:32.47181Z","shell.execute_reply":"2022-01-02T08:01:43.036903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import av\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2022-01-02T08:01:44.791103Z","iopub.execute_input":"2022-01-02T08:01:44.791385Z","iopub.status.idle":"2022-01-02T08:01:45.061334Z","shell.execute_reply.started":"2022-01-02T08:01:44.791353Z","shell.execute_reply":"2022-01-02T08:01:45.060628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def video_to_frame(path,out_path):\n    vidcap = cv2.VideoCapture(path)\n    success,image = vidcap.read()\n    count = 0\n    while success:\n        cv2.imwrite(os.path.join(out_path,\"{}.jpg\".format(count)), image)\n        success,image = vidcap.read()\n        count += 1","metadata":{"execution":{"iopub.status.busy":"2022-01-02T08:01:51.046865Z","iopub.execute_input":"2022-01-02T08:01:51.047124Z","iopub.status.idle":"2022-01-02T08:01:51.05201Z","shell.execute_reply.started":"2022-01-02T08:01:51.047096Z","shell.execute_reply":"2022-01-02T08:01:51.051357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_frames(video_path):\n    frames = []\n    video = av.open(video_path)\n    for frame in video.decode(0):\n        yield frame.to_image()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T08:01:54.859876Z","iopub.execute_input":"2022-01-02T08:01:54.86037Z","iopub.status.idle":"2022-01-02T08:01:54.865469Z","shell.execute_reply.started":"2022-01-02T08:01:54.86033Z","shell.execute_reply":"2022-01-02T08:01:54.86448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = './Dataset'\nos.makedirs(result, exist_ok = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T08:01:57.990522Z","iopub.execute_input":"2022-01-02T08:01:57.991076Z","iopub.status.idle":"2022-01-02T08:01:57.995222Z","shell.execute_reply.started":"2022-01-02T08:01:57.991039Z","shell.execute_reply":"2022-01-02T08:01:57.994315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clear_directory(directorty_path):\n    for sub_f in os.listdir(directorty_path):\n        sub_f = os.path.join(directorty_path,sub_f)\n        for sample_f in os.listdir(sub_f):\n            sample_f = os.path.join(sub_f,sample_f)\n            for f in os.listdir(sample_f):\n                os.remove(os.path.join(sample_f, f))\n        \nclear_directory(result)     ","metadata":{"execution":{"iopub.status.busy":"2022-01-02T08:02:02.79152Z","iopub.execute_input":"2022-01-02T08:02:02.79208Z","iopub.status.idle":"2022-01-02T08:02:02.797253Z","shell.execute_reply.started":"2022-01-02T08:02:02.792037Z","shell.execute_reply":"2022-01-02T08:02:02.796596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.autonotebook import tqdm\n\ndef extract_abnormal_videos_frames(directory_path):\n    for i in tqdm(os.listdir(directory_path)):\n        p1 = os.path.join(directory_path,i)\n        r1 = os.path.join(result,i)\n        if os.path.exists(r1):\n            continue\n        os.makedirs(r1,exist_ok = True)\n        for j in os.listdir(p1):\n            vid_path = os.path.join(p1,j)\n            r2 = os.path.join(r1,j[:-4])\n            os.makedirs(r2,exist_ok = True)\n            for j, frame in enumerate((extract_frames(vid_path))):\n                frame.save(os.path.join(r2, f\"{j}.jpg\"))","metadata":{"execution":{"iopub.status.busy":"2022-01-02T08:02:06.059941Z","iopub.execute_input":"2022-01-02T08:02:06.060191Z","iopub.status.idle":"2022-01-02T08:02:06.235308Z","shell.execute_reply.started":"2022-01-02T08:02:06.060164Z","shell.execute_reply":"2022-01-02T08:02:06.233648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1\"\nextract_abnormal_videos_frames(path)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T08:02:09.350312Z","iopub.execute_input":"2022-01-02T08:02:09.350866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path = \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2\"\n# extract_abnormal_videos_frames(path)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:15:00.624409Z","iopub.execute_input":"2022-01-01T23:15:00.625395Z","iopub.status.idle":"2022-01-01T23:15:00.631697Z","shell.execute_reply.started":"2022-01-01T23:15:00.625203Z","shell.execute_reply":"2022-01-01T23:15:00.630995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_normal_videos_frames(directory_path):\n    #Normal class\n    for i in tqdm(os.listdir(directory_path)):\n        p1 = os.path.join(directory_path,i)\n        r1 = os.path.join(result,i[:-4])\n        if os.path.exists(r1):\n            continue\n        os.makedirs(r1,exist_ok = True)\n        for k, frame in enumerate((extract_frames(p1))):\n            frame.save(os.path.join(r1, f\"{k}.jpg\"))","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:15:00.633414Z","iopub.execute_input":"2022-01-01T23:15:00.633636Z","iopub.status.idle":"2022-01-01T23:15:00.641579Z","shell.execute_reply.started":"2022-01-01T23:15:00.63361Z","shell.execute_reply":"2022-01-01T23:15:00.64085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path = \"../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Normal-Videos-Part-1\"\n# read_normal_videos_frames(path)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:15:00.644864Z","iopub.execute_input":"2022-01-01T23:15:00.645072Z","iopub.status.idle":"2022-01-01T23:15:00.650491Z","shell.execute_reply.started":"2022-01-01T23:15:00.645048Z","shell.execute_reply":"2022-01-01T23:15:00.649782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def crop_image_black_background(img):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n    hh, ww = thresh.shape\n    thresh[hh:hh, 0:ww] = 0\n    white = np.where(thresh == 255)\n    \n    if len(white[0]) <= 1 or len(white[1]) <= 1:\n        return None\n    \n    xmin, ymin, xmax, ymax = np.min(white[1]), np.min(white[0]), np.max(white[1]), np.max(white[0])\n    crop = img[ymin:ymax, xmin:xmax]\n    \n    if (crop.shape[0] < img.shape[0]/2) and (crop.shape[1] < img.shape[1]/2):\n        return None\n    \n    return crop","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:15:00.652052Z","iopub.execute_input":"2022-01-01T23:15:00.652361Z","iopub.status.idle":"2022-01-01T23:15:00.662442Z","shell.execute_reply.started":"2022-01-01T23:15:00.652324Z","shell.execute_reply":"2022-01-01T23:15:00.66178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number\nseq_length = 16\nres = './crime'+str(seq_length)\nos.makedirs(res, exist_ok = True)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:15:00.665624Z","iopub.execute_input":"2022-01-01T23:15:00.665903Z","iopub.status.idle":"2022-01-01T23:15:00.671753Z","shell.execute_reply.started":"2022-01-01T23:15:00.665874Z","shell.execute_reply":"2022-01-01T23:15:00.670971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = './Dataset'\n\ndef preprocess_data(seq_length,path,res):\n    dir = os.listdir(path)\n    for i in tqdm(dir):\n        p1 = os.path.join(path,i)\n        r1 = os.path.join(res,i)\n        os.makedirs(r1,exist_ok = True)\n        for j in os.listdir(p1):\n            p2 = os.path.join(p1,j)\n            r2 = os.path.join(r1,j)\n            l = 0\n            skip_length = int(len(os.listdir(p2))/seq_length)        \n            for m in range(10):\n                k = m\n                while(l!=seq_length):\n                    p3 = os.path.join(p2,str(k) + \".jpg\")            \n                    try:\n                        img = cv2.imread(p3) \n                        print(img)\n                        img = cv2.resize(img,(128,128))\n                    except Exception as e:\n                        print(e)\n                        \n                    if img.shape[0] <=0 or img.shape[1] <= 0:\n                        continue                        \n                    if(k==0):\n                        img1 = img\n                    else:\n                        img1 = np.append(img1,img,axis = 1)\n                    k = k+skip_length\n                    l = l+1    \n                cv2.imwrite(r2 + str(m)+\".jpg\",img1)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:15:00.673643Z","iopub.execute_input":"2022-01-01T23:15:00.673873Z","iopub.status.idle":"2022-01-01T23:15:00.686235Z","shell.execute_reply.started":"2022-01-01T23:15:00.673844Z","shell.execute_reply":"2022-01-01T23:15:00.685455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess_data(seq_length,path,res)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:15:00.687432Z","iopub.execute_input":"2022-01-01T23:15:00.688247Z","iopub.status.idle":"2022-01-01T23:15:00.694838Z","shell.execute_reply.started":"2022-01-01T23:15:00.688205Z","shell.execute_reply":"2022-01-01T23:15:00.69414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom functools import partial\n\n__all__ = ['resnet50', 'resnet101', 'resnet152', 'resnet200']\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4)).zero_()\n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n\n    return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, head_conv=1):\n        super(Bottleneck, self).__init__()\n        if head_conv == 1:\n            self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n            self.bn1 = nn.BatchNorm3d(planes)\n        elif head_conv == 3:\n            self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=(3, 1, 1), bias=False, padding=(1, 0, 0))\n            self.bn1 = nn.BatchNorm3d(planes)\n        else:\n            raise ValueError(\"Unsupported head_conv!\")\n        self.conv2 = nn.Conv3d(\n            planes, planes, kernel_size=(1, 3, 3), stride=(1, stride, stride), padding=(0, 1, 1), bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\ndef get_fine_tuning_parameters(model, ft_begin_index):\n    if ft_begin_index == 0:\n        return model.parameters()\n\n    ft_module_names = []\n    for i in range(ft_begin_index, 5):\n        ft_module_names.append('layer{}'.format(i))\n    ft_module_names.append('fc')\n\n    parameters = []\n    for k, v in model.named_parameters():\n        for ft_module in ft_module_names:\n            if ft_module in k:\n                parameters.append({'params': v})\n                break\n        else:\n            parameters.append({'params': v, 'lr': 0.0})\n\n    return parameters\n\n\nclass SlowFast(nn.Module):\n    def __init__(self, block=Bottleneck, layers=[3, 4, 6, 3], class_num=27, shortcut_type='B', dropout=0.5,\n                 alpha=8, beta=0.125):\n        super(SlowFast, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n\n        self.fast_inplanes = int(64 * beta)\n        fast_inplanes = self.fast_inplanes\n        self.fast_conv1 = nn.Conv3d(3, fast_inplanes, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3),\n                                    bias=False)\n        self.fast_bn1 = nn.BatchNorm3d(8)\n        self.fast_relu = nn.ReLU(inplace=True)\n        self.fast_maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n        self.fast_res1 = self._make_layer_fast(block, 8, layers[0], shortcut_type, head_conv=3)\n        self.fast_res2 = self._make_layer_fast(\n            block, 16, layers[1], shortcut_type, stride=2, head_conv=3)\n        self.fast_res3 = self._make_layer_fast(\n            block, 32, layers[2], shortcut_type, stride=2, head_conv=3)\n        self.fast_res4 = self._make_layer_fast(\n            block, 64, layers[3], shortcut_type, stride=2, head_conv=3)\n\n        self.slow_inplanes = 64\n        slow_inplanes = self.slow_inplanes\n        self.slow_conv1 = nn.Conv3d(3, slow_inplanes, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3),\n                                    bias=False)\n        self.slow_bn1 = nn.BatchNorm3d(64)\n        self.slow_relu = nn.ReLU(inplace=True)\n        self.slow_maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n        self.slow_res1 = self._make_layer_slow(block, 64, layers[0], shortcut_type, head_conv=1)\n        self.slow_res2 = self._make_layer_slow(\n            block, 128, layers[1], shortcut_type, stride=2, head_conv=1)\n        self.slow_res3 = self._make_layer_slow(\n            block, 256, layers[2], shortcut_type, stride=2, head_conv=1)\n        self.slow_res4 = self._make_layer_slow(\n            block, 512, layers[3], shortcut_type, stride=2, head_conv=1)\n\n        self.Tconv1 = nn.Conv3d(8, 16, kernel_size=(5, 1, 1), stride=(alpha, 1, 1), padding=(2, 0, 0), bias=False)\n        self.Tconv2 = nn.Conv3d(32, 64, kernel_size=(5, 1, 1), stride=(alpha, 1, 1), padding=(2, 0, 0), bias=False)\n        self.Tconv3 = nn.Conv3d(64, 128, kernel_size=(5, 1, 1), stride=(alpha, 1, 1), padding=(2, 0, 0), bias=False)\n        self.Tconv4 = nn.Conv3d(128, 256, kernel_size=(5, 1, 1), stride=(alpha, 1, 1), padding=(2, 0, 0), bias=False)\n\n        self.dp = nn.Dropout(dropout)\n        self.fc = nn.Linear(self.fast_inplanes + self.slow_inplanes, class_num)\n\n    def forward(self, input):\n        fast, Tc = self.FastPath(input[:, :, ::2, :, :])\n        slow = self.SlowPath(input[:, :, ::16, :, :], Tc)\n        x = torch.cat([slow, fast], dim=1)\n        x = self.dp(x)\n        x = self.fc(x)\n        return x\n\n    def SlowPath(self, input, Tc):\n        x = self.slow_conv1(input)\n        x = self.slow_bn1(x)\n        x = self.slow_relu(x)\n        x = self.slow_maxpool(x)\n        x = torch.cat([x, Tc[0]], dim=1)\n        x = self.slow_res1(x)\n        x = torch.cat([x, Tc[1]], dim=1)\n        x = self.slow_res2(x)\n        x = torch.cat([x, Tc[2]], dim=1)\n        x = self.slow_res3(x)\n        x = torch.cat([x, Tc[3]], dim=1)\n        x = self.slow_res4(x)\n        x = nn.AdaptiveAvgPool3d(1)(x)\n        x = x.view(-1, x.size(1))\n        return x\n\n    def FastPath(self, input):\n        x = self.fast_conv1(input)\n        x = self.fast_bn1(x)\n        x = self.fast_relu(x)\n        x = self.fast_maxpool(x)\n        Tc1 = self.Tconv1(x)\n        x = self.fast_res1(x)\n        Tc2 = self.Tconv2(x)\n        x = self.fast_res2(x)\n        Tc3 = self.Tconv3(x)\n        x = self.fast_res3(x)\n        Tc4 = self.Tconv4(x)\n        x = self.fast_res4(x)\n        x = nn.AdaptiveAvgPool3d(1)(x)\n        x = x.view(-1, x.size(1))\n        return x, [Tc1, Tc2, Tc3, Tc4]\n\n    def _make_layer_fast(self, block, planes, blocks, shortcut_type, stride=1, head_conv=1):\n        downsample = None\n        if stride != 1 or self.fast_inplanes != planes * block.expansion:\n            if shortcut_type == 'A':\n                downsample = partial(\n                    downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(\n                        self.fast_inplanes,\n                        planes * block.expansion,\n                        kernel_size=1,\n                        stride=(1, stride, stride),\n                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(block(self.fast_inplanes, planes, stride, downsample, head_conv=head_conv))\n        self.fast_inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.fast_inplanes, planes, head_conv=head_conv))\n        return nn.Sequential(*layers)\n\n    def _make_layer_slow(self, block, planes, blocks, shortcut_type, stride=1, head_conv=1):\n        downsample = None\n        if stride != 1 or self.slow_inplanes != planes * block.expansion:\n            if shortcut_type == 'A':\n                downsample = partial(\n                    downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(\n                        self.slow_inplanes + self.slow_inplanes // self.alpha * 2,\n                        planes * block.expansion,\n                        kernel_size=1,\n                        stride=(1, stride, stride),\n                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(block(self.slow_inplanes + self.slow_inplanes // self.alpha * 2, planes, stride, downsample,\n                            head_conv=head_conv))\n        self.slow_inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.slow_inplanes, planes, head_conv=head_conv))\n\n        return nn.Sequential(*layers)\n\n\ndef resnet50(**kwargs):\n    \"\"\"Constructs a ResNet-50 model.\n    \"\"\"\n    model = SlowFast(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef resnet101(**kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n    \"\"\"\n    model = SlowFast(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef resnet152(**kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n    \"\"\"\n    model = SlowFast(Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n\n\ndef resnet200(**kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n    \"\"\"\n    model = SlowFast(Bottleneck, [3, 24, 36, 3], **kwargs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:58:57.913308Z","iopub.execute_input":"2022-01-01T23:58:57.913587Z","iopub.status.idle":"2022-01-01T23:58:57.970276Z","shell.execute_reply.started":"2022-01-01T23:58:57.913555Z","shell.execute_reply":"2022-01-01T23:58:57.969392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OneCycle(object):\n    def __init__(self, nb, max_lr, momentum_vals=(0.95, 0.85), prcnt= 10 , div=10):\n        self.nb = nb\n        self.div = div\n        self.step_len =  int(self.nb * (1- prcnt/100)/2)\n        self.high_lr = max_lr\n        self.low_mom = momentum_vals[1]\n        self.high_mom = momentum_vals[0]\n        self.prcnt = prcnt\n        self.iteration = 0\n        self.lrs = []\n        self.moms = []\n\n    def calc(self):\n        self.iteration += 1\n        lr = self.calc_lr()\n        mom = self.calc_mom()\n        return (lr, mom)\n\n    def calc_lr(self):\n        if self.iteration==self.nb:\n            self.iteration = 0\n            self.lrs.append(self.high_lr/self.div)\n            return self.high_lr/self.div\n        if self.iteration > 2 * self.step_len:\n            ratio = (self.iteration - 2 * self.step_len) / (self.nb - 2 * self.step_len)\n            lr = self.high_lr * ( 1 - 0.99 * ratio)/self.div\n        elif self.iteration > self.step_len:\n            ratio = 1- (self.iteration -self.step_len)/self.step_len\n            lr = self.high_lr * (1 + ratio * (self.div - 1)) / self.div\n        else :\n            ratio = self.iteration/self.step_len\n            lr = self.high_lr * (1 + ratio * (self.div - 1)) / self.div\n        self.lrs.append(lr)\n        return lr\n\n    def calc_mom(self):\n        if self.iteration==self.nb:\n            self.iteration = 0\n            self.moms.append(self.high_mom)\n            return self.high_mom\n        if self.iteration > 2 * self.step_len:\n            mom = self.high_mom\n        elif self.iteration > self.step_len:\n            ratio = (self.iteration -self.step_len)/self.step_len\n            mom = self.low_mom + ratio * (self.high_mom - self.low_mom)\n        else :\n            ratio = self.iteration/self.step_len\n            mom = self.high_mom - ratio * (self.high_mom - self.low_mom)\n        self.moms.append(mom)\n        return mom\ndef update_lr(optimizer, lr):\n    for g in optimizer.param_groups:\n        g['lr'] = lr\ndef update_mom(optimizer, mom):\n    for g in optimizer.param_groups:\n        g['momentum'] = mom","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:58:58.161435Z","iopub.execute_input":"2022-01-01T23:58:58.161739Z","iopub.status.idle":"2022-01-01T23:58:58.177407Z","shell.execute_reply.started":"2022-01-01T23:58:58.161691Z","shell.execute_reply":"2022-01-01T23:58:58.176698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Label file:\ndata_path = './crime'+str(seq_length)\nclasses = os.listdir(data_path)\ndecoder = {}\nfor i in range(len(classes)):\n    decoder[classes[i]] = i\nencoder = {}\nfor i in range(len(classes)):\n    encoder[i] = classes[i]","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:58:58.332349Z","iopub.execute_input":"2022-01-01T23:58:58.332626Z","iopub.status.idle":"2022-01-01T23:58:58.338218Z","shell.execute_reply.started":"2022-01-01T23:58:58.332582Z","shell.execute_reply":"2022-01-01T23:58:58.337375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_train = list()\nid_test = list()\ntest_size = 0.1\npath = './crime'+str(seq_length)\nfor i in os.listdir(path):\n    p1 = os.path.join(path,i)\n    files_dir = os.listdir(p1)\n    len_sample = len(files_dir)\n    test_strat_idx = (len_sample - (len_sample*test_size))\n    for j in range(len_sample):\n        p2 = os.path.join(p1,files_dir[j])\n        if j < test_strat_idx:\n            id_train.append((i,p2))\n        else:\n            id_test.append((i,p2))","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:58:59.375751Z","iopub.execute_input":"2022-01-01T23:58:59.376369Z","iopub.status.idle":"2022-01-01T23:58:59.390244Z","shell.execute_reply.started":"2022-01-01T23:58:59.376327Z","shell.execute_reply":"2022-01-01T23:58:59.389491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(id_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:59:00.166063Z","iopub.execute_input":"2022-01-01T23:59:00.166517Z","iopub.status.idle":"2022-01-01T23:59:00.172996Z","shell.execute_reply.started":"2022-01-01T23:59:00.166477Z","shell.execute_reply":"2022-01-01T23:59:00.172069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(id_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:59:00.486938Z","iopub.execute_input":"2022-01-01T23:59:00.487582Z","iopub.status.idle":"2022-01-01T23:59:00.492538Z","shell.execute_reply.started":"2022-01-01T23:59:00.48754Z","shell.execute_reply":"2022-01-01T23:59:00.491778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\nclass video_dataset(Dataset):\n    def __init__(self,frame_list,sequence_length = 16,transform = None):\n        self.frame_list = frame_list\n        self.transform = transform\n        self.sequence_length = sequence_length\n    def __len__(self):\n        return len(self.frame_list)\n    def __getitem__(self,idx):\n        label,path = self.frame_list[idx]\n        img = cv2.imread(path)\n        seq_img = list()\n        for i in range(16):\n            img1 = img[:,128*i:128*(i+1),:]\n            if(self.transform):\n                img1 = self.transform(img1)\n            seq_img.append(img1)\n        seq_image = torch.stack(seq_img)\n        seq_image = seq_image.reshape(3,16,im_size,im_size)\n        return seq_image,decoder[label]","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:59:01.224005Z","iopub.execute_input":"2022-01-01T23:59:01.224658Z","iopub.status.idle":"2022-01-01T23:59:01.23335Z","shell.execute_reply.started":"2022-01-01T23:59:01.22462Z","shell.execute_reply":"2022-01-01T23:59:01.232617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im_size = 128\nmean = [0.4889, 0.4887, 0.4891]\nstd = [0.2074, 0.2074, 0.2074]\n\n\ntrain_transforms = transforms.Compose([\n                                        transforms.ToPILImage(),\n                                        transforms.Resize((im_size,im_size)),\n                                        transforms.RandomHorizontalFlip(),\n                                        transforms.RandomRotation(degrees=10),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean,std)])\n\n\ntrain_data = video_dataset(id_train,sequence_length = 16,transform = train_transforms)\ntrain_loader = DataLoader(train_data,batch_size = 8,num_workers = 4 ,shuffle = True)\ndataloaders = {'train':train_loader}\nprint(len(train_data))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-02T00:25:45.531789Z","iopub.execute_input":"2022-01-02T00:25:45.532052Z","iopub.status.idle":"2022-01-02T00:25:45.540705Z","shell.execute_reply.started":"2022-01-02T00:25:45.532022Z","shell.execute_reply":"2022-01-02T00:25:45.539893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = resnet50(class_num=4).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T00:25:45.932151Z","iopub.execute_input":"2022-01-02T00:25:45.932639Z","iopub.status.idle":"2022-01-02T00:25:46.21589Z","shell.execute_reply.started":"2022-01-02T00:25:45.932599Z","shell.execute_reply":"2022-01-02T00:25:46.215033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda'\ncls_criterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum = 0.9,weight_decay = 1e-4)\nnum_epochs = 20\nonecyc = OneCycle(len(train_loader)*num_epochs,1e-3)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T00:25:47.64638Z","iopub.execute_input":"2022-01-02T00:25:47.646651Z","iopub.status.idle":"2022-01-02T00:25:47.654208Z","shell.execute_reply.started":"2022-01-02T00:25:47.646619Z","shell.execute_reply":"2022-01-02T00:25:47.653521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs('./weights',exist_ok = True)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-02T00:25:51.399244Z","iopub.execute_input":"2022-01-02T00:25:51.399505Z","iopub.status.idle":"2022-01-02T00:25:51.404797Z","shell.execute_reply.started":"2022-01-02T00:25:51.399475Z","shell.execute_reply":"2022-01-02T00:25:51.403096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.autograd import Variable\niteration = 0\nacc_all = list()\nloss_all = list()\n    \nfor epoch in range(num_epochs):\n    print('')\n    print(f\"--- Epoch {epoch} ---\")\n    phase1 = dataloaders.keys()\n    for phase in phase1:\n        print('')\n        print(f\"--- Phase {phase} ---\")\n        epoch_metrics = {\"loss\": [], \"acc\": []}\n        for batch_i, (X, y) in enumerate(dataloaders[phase]):\n            #iteration = iteration+1\n            image_sequences = Variable(X.to(device), requires_grad=True)\n            labels = Variable(y.to(device), requires_grad=False)\n            optimizer.zero_grad()\n            #model.lstm.reset_hidden_state()\n            predictions = model(image_sequences)\n            loss = cls_criterion(predictions, labels)\n            acc = 100 * (predictions.detach().argmax(1) == labels).cpu().numpy().mean()\n            loss.backward()\n            optimizer.step()\n            epoch_metrics[\"loss\"].append(loss.item())\n            epoch_metrics[\"acc\"].append(acc)\n            if(phase=='train'):\n                lr,mom = onecyc.calc()\n                update_lr(optimizer, lr)\n                update_mom(optimizer, mom)\n            batches_done = epoch * len(dataloaders[phase]) + batch_i\n            batches_left = num_epochs * len(dataloaders[phase]) - batches_done\n            print(\n                    \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f), Acc: %.2f%% (%.2f%%)]\"\n                    % (\n                        epoch,\n                        num_epochs,\n                        batch_i,\n                        len(dataloaders[phase]),\n                        loss.item(),\n                        np.mean(epoch_metrics[\"loss\"]),\n                        acc,\n                        np.mean(epoch_metrics[\"acc\"]),\n                    )\n                )\n\n                # Empty cache\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n        print('')\n        print('{} , acc: {}'.format(phase,np.mean(epoch_metrics[\"acc\"])))\n        torch.save(model.state_dict(),'./weights/c3d_{}.h5'.format(epoch))\n        if(phase=='train'):\n            acc_all.append(np.mean(epoch_metrics[\"acc\"]))\n            loss_all.append(np.mean(epoch_metrics[\"loss\"]))","metadata":{"execution":{"iopub.status.busy":"2022-01-02T00:25:51.933854Z","iopub.execute_input":"2022-01-02T00:25:51.934423Z","iopub.status.idle":"2022-01-02T00:42:39.542498Z","shell.execute_reply.started":"2022-01-02T00:25:51.934381Z","shell.execute_reply":"2022-01-02T00:42:39.54163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# import shutil\n# shutil.make_archive(\"ab_v1\", 'zip', \"./crime16\")","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:57:09.362094Z","iopub.status.idle":"2022-01-01T23:57:09.36269Z","shell.execute_reply.started":"2022-01-01T23:57:09.36244Z","shell.execute_reply":"2022-01-01T23:57:09.362468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = video_dataset(id_test,sequence_length = 16,transform = train_transforms)\ntest_loader = DataLoader(test_data,batch_size = 1,num_workers = 4 ,shuffle = False)\ndataloaders = {'test':test_loader}\nprint(len(test_data))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-02T00:52:36.616635Z","iopub.execute_input":"2022-01-02T00:52:36.617443Z","iopub.status.idle":"2022-01-02T00:52:36.62326Z","shell.execute_reply.started":"2022-01-02T00:52:36.617404Z","shell.execute_reply":"2022-01-02T00:52:36.622547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.autograd import Variable\niteration = 0\nacc_all = list()\nloss_all = list()\n    \nprint('')\nphase1 = dataloaders.keys()\nfor phase in phase1:\n    print('')\n    print(f\"--- Phase {phase} ---\")\n    epoch_metrics = {\"loss\": [], \"acc\": []}\n    for batch_i, (X, y) in enumerate(dataloaders[phase]):\n        #iteration = iteration+1\n        image_sequences = Variable(X.to(device), requires_grad=True)\n        labels = Variable(y.to(device), requires_grad=False)\n#         optimizer.zero_grad()\n        \n        predictions = model(image_sequences)\n        loss = cls_criterion(predictions, labels)\n        acc = 100 * (predictions.detach().argmax(1) == labels).cpu().numpy().mean()\n#         loss.backward()\n#         optimizer.step()\n        epoch_metrics[\"loss\"].append(loss.item())\n        epoch_metrics[\"acc\"].append(acc)       \n        batches_done = epoch * len(dataloaders[phase]) + batch_i\n        batches_left = num_epochs * len(dataloaders[phase]) - batches_done\n        print(\n                \"\\r[Batch %d/%d] [Loss: %f (%f), Acc: %.2f%% (%.2f%%)]\"\n                % (\n                    batch_i,\n                    len(dataloaders[phase]),\n                    loss.item(),\n                    np.mean(epoch_metrics[\"loss\"]),\n                    acc,\n                    np.mean(epoch_metrics[\"acc\"]),\n                )\n            )\n\n            # Empty cache\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    print('')\n    print('{} , acc: {}'.format(phase,np.mean(epoch_metrics[\"acc\"])))","metadata":{"execution":{"iopub.status.busy":"2022-01-02T00:52:37.917528Z","iopub.execute_input":"2022-01-02T00:52:37.918038Z","iopub.status.idle":"2022-01-02T00:52:47.16065Z","shell.execute_reply.started":"2022-01-02T00:52:37.918004Z","shell.execute_reply":"2022-01-02T00:52:47.159337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}